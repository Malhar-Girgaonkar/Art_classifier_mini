{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMep3e4flFqx0x6LQQnxhwL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Malhar-Girgaonkar/Art_classifier_mini/blob/main/Art_classifier_mini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y9DYgzo3eTxr"
      },
      "outputs": [],
      "source": [
        "#Import modules\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mounting google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxsV6ruHelbH",
        "outputId": "ba441c5c-d03b-48e3-931d-9d90a1a5bc4a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make new base directory\n",
        "original_dataset_dir = ''\n",
        "base_dir = '/content/drive/MyDrive/dataset/dataset_updated'\n",
        "os.mkdir(base_dir)"
      ],
      "metadata": {
        "id": "jG2i7OVRsAsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create two folders (train and validation)\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.mkdir(train_dir)\n",
        "\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.mkdir(validation_dir)\n",
        "\n",
        "#Under train folder create 13 folders\n",
        "# ['Western_Medieval', 'Renaissance', 'Rococo', 'Realism', 'Expressionism',\n",
        "#'Japanese_Art', 'Symbolism', 'Neoclassicism', 'Primitivism', 'Romanticism',\n",
        "# 'Academic_Art', 'Baroque', 'Art_Nouveau']\n",
        "\n",
        "train_Academic_Art_dir = os.path.join(train_dir, 'Academic_Art')\n",
        "os.mkdir(train_Academic_Art_dir)\n",
        "\n",
        "train_Art_Nouveau_dir = os.path.join(train_dir, 'Art_Nouveau')\n",
        "os.mkdir(train_Art_Nouveau_dir)\n",
        "\n",
        "train_Baroque_dir = os.path.join(train_dir, 'Baroque')\n",
        "os.mkdir(train_Baroque_dir)\n",
        "\n",
        "train_Expressionism_dir = os.path.join(train_dir, 'Expressionism')\n",
        "os.mkdir(train_Expressionism_dir)\n",
        "\n",
        "train_Japanese_Art_dir = os.path.join(train_dir, 'Japanese_Art')\n",
        "os.mkdir(train_Japanese_Art_dir)\n",
        "\n",
        "train_Neoclassicism_dir = os.path.join(train_dir, 'Neoclassicism')\n",
        "os.mkdir(train_Neoclassicism_dir)\n",
        "\n",
        "train_Primitivism_dir = os.path.join(train_dir, 'Primitivism')\n",
        "os.mkdir(train_Primitivism_dir)\n",
        "\n",
        "train_Realism_dir = os.path.join(train_dir, 'Realism')\n",
        "os.mkdir(train_Realism_dir)\n",
        "\n",
        "train_Renaissance_dir = os.path.join(train_dir, 'Renaissance')\n",
        "os.mkdir(train_Renaissance_dir)\n",
        "\n",
        "train_Rococo_dir = os.path.join(train_dir, 'Rococo')\n",
        "os.mkdir(train_Rococo_dir)\n",
        "\n",
        "train_Romanticism_dir = os.path.join(train_dir, 'Romanticism')\n",
        "os.mkdir(train_Romanticism_dir)\n",
        "\n",
        "train_Symbolism_dir = os.path.join(train_dir, 'Symbolism')\n",
        "os.mkdir(train_Symbolism_dir)\n",
        "\n",
        "train_Western_Medieval_dir = os.path.join(train_dir, 'Western_Medieval')\n",
        "os.mkdir(train_Western_Medieval_dir)\n",
        "\n",
        "#Under validation folder create 13 folders\n",
        "# ['Western_Medieval', 'Renaissance', 'Rococo', 'Realism', 'Expressionism',\n",
        "#'Japanese_Art', 'Symbolism', 'Neoclassicism', 'Primitivism', 'Romanticism',\n",
        "# 'Academic_Art', 'Baroque', 'Art_Nouveau']\n",
        "\n",
        "validation_Academic_Art_dir = os.path.join(validation_dir, 'Academic_Art')\n",
        "os.mkdir(validation_Academic_Art_dir)\n",
        "\n",
        "validation_Art_Nouveau_dir = os.path.join(validation_dir, 'Art_Nouveau')\n",
        "os.mkdir(validation_Art_Nouveau_dir)\n",
        "\n",
        "validation_Baroque_dir = os.path.join(validation_dir, 'Baroque')\n",
        "os.mkdir(validation_Baroque_dir)\n",
        "\n",
        "validation_Expressionism_dir = os.path.join(validation_dir, 'Expressionism')\n",
        "os.mkdir(validation_Expressionism_dir)\n",
        "\n",
        "validation_Japanese_Art_dir = os.path.join(validation_dir, 'Japanese_Art')\n",
        "os.mkdir(validation_Japanese_Art_dir)\n",
        "\n",
        "validation_Neoclassicism_dir = os.path.join(validation_dir, 'Neoclassicism')\n",
        "os.mkdir(validation_Neoclassicism_dir)\n",
        "\n",
        "validation_Primitivism_dir = os.path.join(validation_dir, 'Primitivism')\n",
        "os.mkdir(validation_Primitivism_dir)\n",
        "\n",
        "validation_Realism_dir = os.path.join(validation_dir, 'Realism')\n",
        "os.mkdir(validation_Realism_dir )\n",
        "\n",
        "validation_Renaissance_dir = os.path.join(validation_dir, 'Renaissance')\n",
        "os.mkdir(validation_Renaissance_dir)\n",
        "\n",
        "validation_Rococo_dir = os.path.join(validation_dir, 'Rococo')\n",
        "os.mkdir(validation_Rococo_dir)\n",
        "\n",
        "train_Romanticism_dir = os.path.join(validation_dir, 'Romanticism')\n",
        "os.mkdir(train_Romanticism_dir)\n",
        "\n",
        "train_Symbolism_dir = os.path.join(validation_dir, 'Symbolism')\n",
        "os.mkdir(train_Symbolism_dir)\n",
        "\n",
        "train_Western_Medieval_dir = os.path.join(validation_dir, 'Western_Medieval')\n",
        "os.mkdir(train_Western_Medieval_dir)\n"
      ],
      "metadata": {
        "id": "wZ-unabYr-fR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Spliting dataset into training testing validation\n",
        "\n",
        "def split_data(SOURCE, TRAINING, VALIDATION, SPLIT_SIZE):\n",
        "    files = []\n",
        "    for filename in os.listdir(SOURCE):\n",
        "        file = SOURCE + filename\n",
        "        if os.path.getsize(file) > 0:\n",
        "            files.append(filename)\n",
        "        else:\n",
        "            print(filename + \" is zero length, so ignoring.\")\n",
        "\n",
        "    training_length = int(len(files) * SPLIT_SIZE)\n",
        "    valid_length = int(len(files) - training_length)\n",
        "    shuffled_set = random.sample(files, len(files))\n",
        "    training_set = shuffled_set[0:training_length]\n",
        "    valid_set = shuffled_set[training_length:]\n",
        "\n",
        "    for filename in training_set:\n",
        "        this_file = SOURCE + filename\n",
        "        destination = TRAINING + filename\n",
        "        shutil.copyfile(this_file, destination)\n",
        "\n",
        "    for filename in valid_set:\n",
        "        this_file = SOURCE + filename\n",
        "        destination = VALIDATION + filename\n",
        "        shutil.copyfile(this_file, destination)"
      ],
      "metadata": {
        "id": "gt9X5w_Cr2A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#List directories to be used in spliting\n",
        "\n",
        "drawings_SOURCE_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Datasets/Academic_Art/'\n",
        "TRAINING_drawings_DIR = '/content/drive/MyDrive/dataset/dataset_updated/training_set/drawings/'\n",
        "VALID_drawings_DIR = '/content/drive/MyDrive/dataset/dataset_updated/validation_set/drawings/'\n",
        "\n",
        "engraving_SOURCE_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Datasets/Art_Nouveau/'\n",
        "TRAINING_engraving_DIR = '/content/drive/MyDrive/dataset/dataset_updated/training_set/engraving/'\n",
        "VALID_engraving_DIR = '/content/drive/MyDrive/dataset/dataset_updated/validation_set/engraving/'\n",
        "\n",
        "iconography_SOURCE_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Datasets/Baroque/'\n",
        "TRAINING_iconography_DIR = '/content/drive/MyDrive/dataset/dataset_updated/training_set/iconography/'\n",
        "VALID_iconography_DIR = '/content/drive/MyDrive/dataset/dataset_updated/validation_set/iconography/'\n",
        "\n",
        "painting_SOURCE_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Datasets/Expressionism/'\n",
        "TRAINING_painting_DIR = '/content/drive/MyDrive/dataset/dataset_updated/training_set/painting/'\n",
        "VALID_painting_DIR = '/content/drive/MyDrive/dataset/dataset_updated/validation_set/painting/'\n",
        "\n",
        "sculpture_SOURCE_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Datasets/Japanese_Art/'\n",
        "TRAINING_sculpture_DIR = '/content/drive/MyDrive/dataset/dataset_updated/training_set/sculpture/'\n",
        "VALID_sculpture_DIR = '/content/drive/MyDrive/dataset/dataset_updated/validation_set/sculpture/'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FR3BWuL-ewPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Seting split of 80,20 for training,validation of size 21.6gb,5.4gb approx\n",
        "#it took me 48m 1s to run this script\n",
        "\n",
        "split_size = .85\n",
        "\n",
        "split_data(Academic_Art_SOURCE_DIR, TRAINING_Academic_Art_DIR, VALID_Academic_Art_DIR , split_size)\n",
        "\n",
        "split_data(Art_Nouveau_SOURCE_DIR, TRAINING_Art_Nouveau_DIR, VALID_Art_Nouveau_DIR , split_size)\n",
        "\n",
        "split_data(Baroque_SOURCE_DIR, TRAINING_Baroque_DIR, VALID_Baroque_DIR , split_size)\n",
        "\n",
        "split_data(Expressionism_SOURCE_DIR, TRAINING_Expressionism_DIR, VALID_Expressionism_DIR , split_size)\n",
        "\n",
        "split_data(Japanese_Art_SOURCE_DIR, TRAINING_Japanese_Art_DIR, VALID_Japanese_Art_DIR , split_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "hIR9nVVQexpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Looking at data in training part\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.image import imread\n",
        "import pathlib\n",
        "\n",
        "image_folder = ['drawings', 'engraving', 'iconography', 'painting', 'sculpture']\n",
        "\n",
        "nimgs = {}\n",
        "for i in image_folder:\n",
        "    nimages = len(os.listdir('/content/drive/MyDrive/dataset/dataset_updated/training_set/'+i+'/'))\n",
        "    nimgs[i]=nimages\n",
        "plt.figure(figsize=(18, 6))\n",
        "plt.bar(range(len(nimgs)), list(nimgs.values()), align='center')\n",
        "plt.xticks(range(len(nimgs)), list(nimgs.keys()))\n",
        "plt.title('Distribution of different classes in Training Dataset')\n",
        "plt.show()\n",
        "print(\"\\n\\n\")\n",
        "#print no of images in each category\n",
        "for i in image_folder:\n",
        "    print('Training {} images are: '.format(i)+str(len(os.listdir('/content/drive/MyDrive/dataset/dataset_updated/training_set/'+i+'/'))))"
      ],
      "metadata": {
        "id": "10B5C0SWe2dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Looking data in validation part\n",
        "nimgs = {}\n",
        "for i in image_folder:\n",
        "    nimages = len(os.listdir('/content/drive/MyDrive/dataset/dataset_updated/validation_set/'+i+'/'))\n",
        "    nimgs[i]=nimages\n",
        "plt.figure(figsize=(18, 6))\n",
        "plt.bar(range(len(nimgs)), list(nimgs.values()), align='center')\n",
        "plt.xticks(range(len(nimgs)), list(nimgs.keys()))\n",
        "plt.title('Distribution of different classes in Validation Dataset')\n",
        "plt.show()\n",
        "print(\"\\n\\n\")\n",
        "#print no of images in each category\n",
        "for i in image_folder:\n",
        "    print('Testing {} images are: '.format(i)+str(len(os.listdir('/content/drive/MyDrive/dataset/dataset_updated/validation_set/'+i+'/'))))"
      ],
      "metadata": {
        "id": "v-iG6yVae54E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING DNN\n"
      ],
      "metadata": {
        "id": "unu0o6nie_uC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import deep learning based modules\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "eLZbd2mUfDel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if gpu is allocated\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "len(gpus)"
      ],
      "metadata": {
        "id": "4hR0FRR2fGNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Limit gpu usage to reasonable level and avoid OOM errors\n",
        "\n",
        "# List available physical GPUs\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "\n",
        "# Set memory growth for each GPU\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ],
      "metadata": {
        "id": "R3W-c__UfIdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting image height and width and batch size\n",
        "img_width=256; img_height=256\n",
        "batch_size=64"
      ],
      "metadata": {
        "id": "P4swhxRkfK9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set training directory path and create a pipeline as datagen\n",
        "TRAINING_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/train/'\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1/255.0,\n",
        "                                   rotation_range=30,\n",
        "                                   zoom_range=0.4,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(img_height, img_width)\n",
        "                                                    )"
      ],
      "metadata": {
        "id": "XuCpm63QfNpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set validation directory path and pipeline\n",
        "VALIDATION_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/validation/'\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale = 1/255.0)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                              batch_size=batch_size,\n",
        "                                                              class_mode='categorical',\n",
        "                                                              target_size=(img_height, img_width)\n",
        "                                                             )"
      ],
      "metadata": {
        "id": "KNDjCKJofPzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preventing overfitting using Earlystopping and saving best model using ModelCheckpoint\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
        "# autosave best Model in location :\n",
        "best_model_file = '/content/drive/MyDrive/Art Classifier Dataset/Best_Models/CNN_aug_best_weights.h5'\n",
        "best_model = ModelCheckpoint(best_model_file, monitor='val_acc', verbose = 1, save_best_only = True)"
      ],
      "metadata": {
        "id": "7CC-Bkz4fS_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#********************Convulation Neural Network Training starts here********************\n",
        "#CNN structure NOTE:last dense layer value must match number of classes,ie=13\n",
        "model = Sequential([\n",
        "    Conv2D(16, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)), MaxPooling2D(2, 2),\n",
        "    Conv2D(32, (3, 3), activation='relu'), MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(256, (3, 3), activation='relu'),\n",
        "    Conv2D(256, (3, 3), activation='relu'),\n",
        "    Conv2D(256, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(13, activation='softmax')\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "FO9GBYDtfUlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compiling model\n",
        "model.compile(optimizer='Adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics =['accuracy'])"
      ],
      "metadata": {
        "id": "IkLnjtSffWzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This part is actually training of model and takes long time consider before running\n",
        "#here we start with moderate epoch of 50 or 100\n",
        "history = model.fit_generator(train_generator,\n",
        "                              epochs=20,\n",
        "                              verbose=1,\n",
        "                              validation_data=validation_generator,\n",
        "                              callbacks = [early_stopping,best_model]\n",
        "                              )"
      ],
      "metadata": {
        "id": "3x4AlKtSfYl7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}